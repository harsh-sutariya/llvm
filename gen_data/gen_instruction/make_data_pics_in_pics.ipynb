{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3be7d82-7b20-4ee0-b06c-9ba2d08c239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3826f3bb-7dbe-4074-a017-12a9e5dfc378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62461\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/petrelfs/liuziyu/RLHF/make_data/data_randomsample_randompic_45k/sft_data/llava62k_v4.json\", 'r', encoding='utf-8') as file:\n",
    "    dpo_data = json.load(file)\n",
    "print(len(dpo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a15e3-ce0a-4de7-88fb-45bdfa42e401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '000000064933',\n",
       " 'image': 'coco/train2017/000000064933.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nWhat is the woman doing in the image?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The woman in the image is playing tennis, standing on a tennis court, and about to hit a tennis ball with her racket.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'How many images are there of the woman with the tennis racket?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'There is a single image that features four photos of the woman in different poses with a tennis racket.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Is the woman standing in the field or on the tennis court?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The woman is standing on a tennis court while playing with a tennis ball.'},\n",
       "  {'from': 'human', 'value': 'What is the woman using to play tennis?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The woman is using a tennis racket to play tennis.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you provide a brief overview of the sport of tennis?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Tennis is a popular global sport that is played between either two individuals (singles) or two teams of two players each (doubles). Originating in England during the late 19th century, the sport is now enjoyed by millions of people worldwide, both recreationally and professionally.\\n\\nThe objective of the game is to strike a hollow rubber ball, known as a tennis ball, over a net that divides the tennis court into two equal halves. Players use a tennis racket to hit the ball, aiming to make it bounce within the opponent's court area, with the goal of winning points. Players score points when their opponent is unable to return the ball within the designated boundaries or commits a playing error.\\n\\nTennis courts can be indoor or outdoor and come in different surfaces, such as grass, clay, or hard court. The court dimensions, markings, and the net height are standardized by the International Tennis Federation (ITF), which governs the sport's rules and regulations.\\n\\nA tennis match consists of sets, and each set consists of games. In a game, one player serves the ball, attempting to win points, while the other player receives the serve and tries to return it successfully. To win a set, a player must reach a specific number of games won, with a two-game advantage over the opponent. The number of sets required to win a match varies depending on the competition or tournament.\\n\\nTennis has four major annual tournaments known as the Grand Slam events, the Australian Open, the French Open, Wimbledon, and the US Open, each played on different surfaces. These prestigious tournaments attract top players from around the world and captivate tennis fans globally.\"}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_data[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293ddd4b-44a3-41c5-b79a-0b1430ef8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196\n",
      "2196\n",
      "2196\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "textvqa 数据集全部都是以 <image>\\nProvide a one-sentence caption for the provided image.\\n 作为问题的开头\n",
    "\"\"\"\n",
    "count = 0\n",
    "count_convs = 0\n",
    "textvqa_data = []\n",
    "for data in dpo_data:\n",
    "    if data[\"image\"].split('/')[0] == 'textvqa':\n",
    "        textvqa_data.append(data)\n",
    "        if data[\"conversations\"][0]['value'].startswith('<image>\\n'):\n",
    "            count+=1\n",
    "        if len(data[\"conversations\"])==2:\n",
    "            count_convs += 1\n",
    "print(count)\n",
    "print(count_convs)\n",
    "print(len(textvqa_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f509a8d-81e7-41af-94e5-d51790eef1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36410\n",
      "36410\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "coco 数据集全部都是以 <image>\\n 或者是 <image> 作为问题的开头或者是结尾\n",
    "\"\"\"\n",
    "count = 0\n",
    "coco_data = []\n",
    "for data in dpo_data:\n",
    "    if data[\"image\"].split('/')[0] == 'coco':\n",
    "        coco_data.append(data)\n",
    "        if data[\"conversations\"][0]['value'].startswith('<image>\\n'):\n",
    "            count+=1\n",
    "        if data[\"conversations\"][0]['value'].endswith('<image>'):\n",
    "            count+=1\n",
    "print(count)\n",
    "print(len(coco_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4763711-d06d-422a-b1df-0ba7b8f1b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ocr_vqa 全部都是以 <image>\\n 作为问题的开头\n",
    "\"\"\"\n",
    "count = 0\n",
    "ocr_vqa_data = []\n",
    "for data in dpo_data:\n",
    "    if data[\"image\"].split('/')[0] == 'ocr_vqa':\n",
    "        ocr_vqa_data.append(data)\n",
    "        if data[\"conversations\"][0]['value'].startswith('<image>\\n'):\n",
    "            count+=1\n",
    "print(count)\n",
    "print(len(ocr_vqa_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc50bb3-ff3b-43d8-80a7-78207212aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7214\n",
      "7214\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "gpa 全部都是以 <image>\\n 作为问题的开头\n",
    "\"\"\"\n",
    "count = 0\n",
    "statistic = 0\n",
    "gqa_data = []\n",
    "for data in dpo_data:\n",
    "    if data[\"image\"].split('/')[0] == 'gqa':\n",
    "        gqa_data.append(data)\n",
    "        if data[\"conversations\"][0]['value'].startswith('<image>\\n') and data[\"conversations\"][0]['value'].endswith('Answer the question using a single word or phrase.'):\n",
    "            count+=1\n",
    "print(count)\n",
    "print(len(gqa_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610ab78-e420-4cfa-8e79-af366c5fa53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c2537-31b4-4530-9cbd-67fd795a0e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8cec9-c5e6-4c07-8192-8102535a0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5ac0e-4552-4066-9c68-b4aa203a5b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faadb66-dd79-48a9-ae37-6a26403e68e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fffe5-c28b-473e-bb00-89318917e892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095ed98-f3d3-4ac5-96f4-d9c81949fb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b735a950-1f15-42f0-a609-a3170e99e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_companions(input_str, str_list, count=1):\n",
    "    filtered_list = [s for s in str_list if s != input_str]\n",
    "    return random.sample(filtered_list, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94193b9-fda2-48be-b502-8c82bdec8b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196\n",
      "36410\n",
      "8000\n",
      "7214\n",
      "53820\n"
     ]
    }
   ],
   "source": [
    "textvqa_image = []\n",
    "for data in textvqa_data:\n",
    "    textvqa_image.append(data[\"image\"])\n",
    "print(len(textvqa_image))\n",
    "\n",
    "coco_image = []\n",
    "for data in coco_data:\n",
    "    coco_image.append(data[\"image\"])\n",
    "print(len(coco_image))\n",
    "\n",
    "ocr_vqa_image = []\n",
    "for data in ocr_vqa_data:\n",
    "    ocr_vqa_image.append(data[\"image\"])\n",
    "print(len(ocr_vqa_image))\n",
    "\n",
    "# vq_image = []\n",
    "# for data in vg_data:\n",
    "#     vq_image.append(data[\"image\"])\n",
    "# print(len(vq_image))\n",
    "\n",
    "gqa_image = []\n",
    "for data in gqa_data:\n",
    "    gqa_image.append(data[\"image\"])\n",
    "print(len(gqa_image))\n",
    "\n",
    "all_images = textvqa_image + coco_image + ocr_vqa_image + gqa_image\n",
    "print(len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b1edbc8-3b64-4bec-b272-14fbf4ece6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_and_paste_image(image1_path, image2_path, output_path):\n",
    "    # 打开两张图片\n",
    "    image1 = Image.open(image1_path)\n",
    "    image2 = Image.open(image2_path)\n",
    "\n",
    "    # 获取第二张图片的尺寸\n",
    "    image2_width, image2_height = image2.size\n",
    "\n",
    "    # 计算将第一张图片缩小为第二张图片大小的一半，同时保持比例\n",
    "    aspect_ratio = image1.width / image1.height\n",
    "    if image2_width * 0.7 / aspect_ratio <= image2_height * 0.7:\n",
    "        new_width = int(image2_width * 0.7)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = int(image2_height * 0.7)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    image1_resized = image1.resize((new_width, new_height))\n",
    "\n",
    "    # 计算将第一张图片放在第二张图片中间的坐标\n",
    "    paste_x = (image2_width - new_width) // 2\n",
    "    paste_y = (image2_height - new_height) // 2\n",
    "\n",
    "    # 在第二张图片的中间粘贴第一张图片\n",
    "    image2.paste(image1_resized, (paste_x, paste_y))\n",
    "\n",
    "    image2 = image2.convert('RGB')\n",
    "\n",
    "    # 保存结果\n",
    "    image2.save(output_path)\n",
    "\n",
    "# 调用函数\n",
    "# resize_and_paste_image('/mnt/petrelfs/liuziyu/RLHF/Observation/pics/bear.png', '/mnt/petrelfs/liuziyu/RLHF/Observation/pics/medicine.png', 'output_image1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd67f61-e64f-4887-ab83-8a36d4021c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt = [\n",
    "    'For the image in the center',\n",
    "    'Regarding the central image',\n",
    "    'Concerning the middle image',\n",
    "    'Pertaining to the central picture',\n",
    "    'With respect to the image in the middle',\n",
    "    'As for the image at the center',\n",
    "    'Focusing on the central image',\n",
    "    'In relation to the middle picture',\n",
    "    'When it comes to the central image',\n",
    "    'With regards to the image in the center',\n",
    "    'For the image located in the center',\n",
    "    'About the central image',\n",
    "    'Referring to the image in the middle',\n",
    "    'In terms of the central picture',\n",
    "    'For the middle picture',\n",
    "    'Regarding the image in the middle',\n",
    "    'Concerning the image at the center',\n",
    "    'Pertaining to the image in the center',\n",
    "    'With respect to the central picture',\n",
    "    'As for the image in the middle',\n",
    "    'Focusing on the middle image',\n",
    "    'In relation to the central image',\n",
    "    'When it comes to the image in the center',\n",
    "    'With regards to the central picture',\n",
    "    'For the picture in the center',\n",
    "    'Regarding the middle image',\n",
    "    'Concerning the picture in the middle',\n",
    "    'Pertaining to the middle image',\n",
    "    'With respect to the image at the center',\n",
    "    'As for the central picture',\n",
    "    'Focusing on the image in the center',\n",
    "    'In relation to the image in the middle',\n",
    "    'When it comes to the central picture',\n",
    "    'With regards to the middle image',\n",
    "    'For the central picture',\n",
    "    'Regarding the image in the center',\n",
    "    'Concerning the image in the middle',\n",
    "    'Pertaining to the central image',\n",
    "    'With respect to the middle picture',\n",
    "    'As for the picture in the center',\n",
    "    'Focusing on the central picture',\n",
    "    'In relation to the middle image',\n",
    "    'When it comes to the image at the center',\n",
    "    'With regards to the image in the middle',\n",
    "    'For the image at the center',\n",
    "    'Regarding the central picture',\n",
    "    'Concerning the central image',\n",
    "    'Pertaining to the picture in the center',\n",
    "    'With respect to the central image',\n",
    "    'As for the image in the center', \n",
    "    'Regarding the central subplot',\n",
    "    'Concerning the central subplot',\n",
    "    'Pertaining to the central subplot',\n",
    "    'With respect to the central subplot',\n",
    "    'As for the central subplot',\n",
    "    'Focusing on the central subplot',\n",
    "    'In relation to the central subplot',\n",
    "    'When it comes to the central subplot',\n",
    "    'With regards to the central subplot',\n",
    "    'For the subplot in the center',\n",
    "    'About the central subplot',\n",
    "    'Referring to the central subplot',\n",
    "    'In terms of the central subplot',\n",
    "    'For the subplot at the center',\n",
    "    'Regarding the subplot in the center',\n",
    "    'Concerning the subplot in the middle',\n",
    "    'Pertaining to the subplot in the center',\n",
    "    'With respect to the subplot at the center',\n",
    "    'As for the subplot in the middle',\n",
    "    'Focusing on the subplot at the center',\n",
    "    'In relation to the subplot in the middle',\n",
    "    'When it comes to the subplot in the center',\n",
    "    'With regards to the subplot at the center',\n",
    "    'For the subplot located in the center',\n",
    "    'Regarding the middle subplot',\n",
    "    'Concerning the subplot in the center',\n",
    "    'Pertaining to the middle subplot',\n",
    "    'With respect to the subplot in the middle',\n",
    "    'As for the middle subplot',\n",
    "    'Focusing on the middle subplot',\n",
    "    'In relation to the middle subplot',\n",
    "    'When it comes to the middle subplot',\n",
    "    'With regards to the middle subplot',\n",
    "    'For the central plot',\n",
    "    'Regarding the plot in the middle',\n",
    "    'Concerning the plot at the center',\n",
    "    'Pertaining to the central plot',\n",
    "    'With respect to the central plot',\n",
    "    'As for the plot in the center',\n",
    "    'Focusing on the plot in the middle',\n",
    "    'In relation to the plot at the center',\n",
    "    'When it comes to the plot in the middle',\n",
    "    'With regards to the central plot',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa68c45d-6aa9-4501-82a7-3f035a85a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196it [04:22,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196\n",
      "Data has been written to ./mix_textvqa_data.json\n"
     ]
    }
   ],
   "source": [
    "mix_textvqa_data = []\n",
    "for index, data in tqdm(enumerate(textvqa_data)):\n",
    "    \n",
    "    image_list = find_companions(data['image'], textvqa_image, 1)\n",
    "    in_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + data['image']\n",
    "    out_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + image_list[0]\n",
    "\n",
    "    output_path = f'/mnt/hwfile/mllm/liuziyu/RLHF_data/RLHF_pics_in_pics/textvqa_{index}.jpg'\n",
    "    resize_and_paste_image(in_pics, out_pics, output_path)\n",
    "\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>\\n', '')\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>', '')\n",
    "    for convs_index, convs in enumerate(data[\"conversations\"]):\n",
    "        if convs['from'] == 'human':\n",
    "            if convs_index == 0:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = '<image>\\n' + local_meta_prompt + ', ' + convs['value'][0].lower() + convs['value'][1:]\n",
    "            else:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = local_meta_prompt + ', ' +  convs['value'][0].lower() + convs['value'][1:]\n",
    "\n",
    "    data['image'] = output_path\n",
    "    mix_textvqa_data.append(data)\n",
    "\n",
    "print(len(mix_textvqa_data))\n",
    "output_json_path = \"./mix_textvqa_data.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(mix_textvqa_data, json_file, indent=4)\n",
    "print(f\"Data has been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b47a52-a0c0-4a9c-b187-9e94fdd48170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [06:15, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "Data has been written to ./mix_ocr_vqa_data.json\n"
     ]
    }
   ],
   "source": [
    "mix_ocr_vqa_data = []\n",
    "for index, data in tqdm(enumerate(ocr_vqa_data)):\n",
    "    \n",
    "    image_list = find_companions(data['image'], ocr_vqa_image, 1)\n",
    "    in_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + data['image']\n",
    "    out_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + image_list[0]\n",
    "\n",
    "    output_path = f'/mnt/hwfile/mllm/liuziyu/RLHF_data/RLHF_pics_in_pics/ocrvqa_{index}.jpg'\n",
    "    resize_and_paste_image(in_pics, out_pics, output_path)\n",
    "\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>\\n', '')\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>', '')\n",
    "    for convs_index, convs in enumerate(data[\"conversations\"]):\n",
    "        if convs['from'] == 'human':\n",
    "            if convs_index == 0:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = '<image>\\n' + local_meta_prompt + ', ' + convs['value'][0].lower() + convs['value'][1:]\n",
    "            else:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = local_meta_prompt + ', ' +  convs['value'][0].lower() + convs['value'][1:]\n",
    "\n",
    "    data['image'] = output_path\n",
    "    mix_ocr_vqa_data.append(data)\n",
    "\n",
    "print(len(mix_ocr_vqa_data))\n",
    "output_json_path = \"./mix_ocr_vqa_data.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(mix_ocr_vqa_data, json_file, indent=4)\n",
    "print(f\"Data has been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc7ff9d-2f9b-4d27-903a-8f0a69b5aff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7214it [07:37, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7214\n",
      "Data has been written to ./mix_gqa_data.json\n"
     ]
    }
   ],
   "source": [
    "mix_gqa_data = []\n",
    "for index, data in tqdm(enumerate(gqa_data)):\n",
    "    \n",
    "    image_list = find_companions(data['image'], gqa_image, 1)\n",
    "    in_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + data['image']\n",
    "    out_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + image_list[0]\n",
    "\n",
    "    output_path = f'/mnt/hwfile/mllm/liuziyu/RLHF_data/RLHF_pics_in_pics/gqa_{index}.jpg'\n",
    "    resize_and_paste_image(in_pics, out_pics, output_path)\n",
    "\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>\\n', '')\n",
    "    data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>', '')\n",
    "    for convs_index, convs in enumerate(data[\"conversations\"]):\n",
    "        if convs['from'] == 'human':\n",
    "            if convs_index == 0:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = '<image>\\n' + local_meta_prompt + ', ' + convs['value'][0].lower() + convs['value'][1:]\n",
    "            else:\n",
    "                local_meta_prompt = random.choice(meta_prompt)\n",
    "                convs['value'] = local_meta_prompt + ', ' +  convs['value'][0].lower() + convs['value'][1:]\n",
    "\n",
    "    data['image'] = output_path\n",
    "    mix_gqa_data.append(data)\n",
    "\n",
    "print(len(mix_gqa_data))\n",
    "output_json_path = \"./mix_gqa_data.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(mix_gqa_data, json_file, indent=4)\n",
    "print(f\"Data has been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592b65cb-0ba7-485b-b74f-c8fa48ca73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def contains_bbox(s):\n",
    "    bbox_pattern = r'\\[\\s*\\d+(\\.\\d+)?,\\s*\\d+(\\.\\d+)?,\\s*\\d+(\\.\\d+)?,\\s*\\d+(\\.\\d+)?\\s*\\]'\n",
    "    match = re.search(bbox_pattern, s)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8892ff72-5e74-4040-8800-162f90f28819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36410it [39:46, 15.26it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31565\n",
      "Data has been written to ./mix_coco_data.json\n"
     ]
    }
   ],
   "source": [
    "mix_coco_data = []\n",
    "for index, data in tqdm(enumerate(coco_data)):\n",
    "    choose_flase = True\n",
    "    for conv in data[\"conversations\"]:\n",
    "        conv_value = conv[\"value\"]\n",
    "        if contains_bbox(conv_value):\n",
    "            choose_flase = False\n",
    "    if choose_flase == True:\n",
    "        image_list = find_companions(data['image'], coco_image, 1)\n",
    "        in_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + data['image']\n",
    "        out_pics = '/mnt/petrelfs/liuziyu/V3Det/LLaVA/scripts/v1_5/data/cl_data/' + image_list[0]\n",
    "    \n",
    "        output_path = f'/mnt/hwfile/mllm/liuziyu/RLHF_data/RLHF_pics_in_pics/coco_{index}.jpg'\n",
    "        resize_and_paste_image(in_pics, out_pics, output_path)\n",
    "    \n",
    "        data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>\\n', '')\n",
    "        data[\"conversations\"][0]['value'] = data[\"conversations\"][0]['value'].replace('<image>', '')\n",
    "        for convs_index, convs in enumerate(data[\"conversations\"]):\n",
    "            if convs['from'] == 'human':\n",
    "                if convs_index == 0:\n",
    "                    local_meta_prompt = random.choice(meta_prompt)\n",
    "                    convs['value'] = '<image>\\n' + local_meta_prompt + ', ' + convs['value'][0].lower() + convs['value'][1:]\n",
    "                else:\n",
    "                    local_meta_prompt = random.choice(meta_prompt)\n",
    "                    convs['value'] = local_meta_prompt + ', ' +  convs['value'][0].lower() + convs['value'][1:]\n",
    "    \n",
    "        data['image'] = output_path\n",
    "        mix_coco_data.append(data)\n",
    "\n",
    "print(len(mix_coco_data))\n",
    "output_json_path = \"./mix_coco_data.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(mix_coco_data, json_file, indent=4)\n",
    "print(f\"Data has been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f185ca88-90c1-41e6-b11f-eb880d72fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31565\n",
      "7214\n",
      "8000\n",
      "2196\n",
      "48975\n",
      "Data has been written to ./pics_in_pics_49k.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/petrelfs/liuziyu/RLHF/make_data/data_randomsample_randompic_45k/mix_coco_data.json\", 'r', encoding='utf-8') as file:\n",
    "    mix_coco_data = json.load(file)\n",
    "print(len(mix_coco_data))\n",
    "with open(\"/mnt/petrelfs/liuziyu/RLHF/make_data/data_randomsample_randompic_45k/mix_gqa_data.json\", 'r', encoding='utf-8') as file:\n",
    "    mix_gqa_data = json.load(file)\n",
    "print(len(mix_gqa_data))\n",
    "with open(\"/mnt/petrelfs/liuziyu/RLHF/make_data/data_randomsample_randompic_45k/mix_ocr_vqa_data.json\", 'r', encoding='utf-8') as file:\n",
    "    mix_ocr_vqa_data = json.load(file)\n",
    "print(len(mix_ocr_vqa_data))\n",
    "with open(\"/mnt/petrelfs/liuziyu/RLHF/make_data/data_randomsample_randompic_45k/mix_textvqa_data.json\", 'r', encoding='utf-8') as file:\n",
    "    mix_textvqa_data = json.load(file)\n",
    "print(len(mix_textvqa_data))\n",
    "pics_in_pics_62k = mix_coco_data+mix_gqa_data+mix_ocr_vqa_data+mix_textvqa_data\n",
    "print(len(pics_in_pics_62k))\n",
    "output_json_path = \"./pics_in_pics_49k.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(pics_in_pics_62k, json_file, indent=4)\n",
    "print(f\"Data has been written to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd866bb-c0fe-461d-a178-e126936e148a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
