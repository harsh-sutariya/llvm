{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee7b122-10bc-442a-a06c-9aa628271975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List, Any, Tuple, Union\n",
    "from accelerate.utils import DistributedType\n",
    "from logzero import logger\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "from packaging import version\n",
    "IS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse('0.14')\n",
    "\n",
    "# from llava.constants import IGNORE_INDEX, X_TOKEN_INDEX, DEFAULT_X_TOKEN, DEFAULT_X_START_TOKEN, DEFAULT_X_END_TOKEN\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVADPOTrainer\n",
    "from utils import load_jsonl, load_json\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "local_rank = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2203f7a2-c401-4297-bba5-6d40679a5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    ### 此处多了一个X\n",
    "    X: Optional[List[str]] = field(default=None)\n",
    "    ### vision_tower 变为 image_tower和video_tower\n",
    "    image_tower: Optional[str] = field(default=None)\n",
    "    video_tower: Optional[str] = field(default=None)\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_x_start_end: bool = field(default=False)\n",
    "    mm_use_x_patch_token: bool = field(default=True)\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "    image_grid_pinpoints: Optional[str] = field(default=None)\n",
    "    video_folder: Optional[str] = field(default=None)\n",
    "    training_modal: Optional[str] = field(default='video')\n",
    "    num_sample: Optional[int] = field(default=None)\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n",
    "\n",
    "    fix_vit: bool = True\n",
    "    dpo_alpha: float = field(default=1.0)\n",
    "    beta: float = field(default=0.1)\n",
    "    gamma: float = field(default=1.0)\n",
    "    generate_during_eval: bool = field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d81552-08ae-4102-a323-beebaefe6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def maybe_zero_3(param, ignore_status=False, name=None):\n",
    "    from deepspeed import zero\n",
    "    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "    if hasattr(param, \"ds_id\"):\n",
    "        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n",
    "            if not ignore_status:\n",
    "                logging.warning(f\"{name}: param.ds_status != ZeroParamStatus.NOT_AVAILABLE: {param.ds_status}\")\n",
    "        with zero.GatheredParameters([param]):\n",
    "            param = param.data.detach().cpu().clone()\n",
    "    else:\n",
    "        param = param.detach().cpu().clone()\n",
    "    return param\n",
    "\n",
    "\n",
    "# Borrowed from peft.utils.get_peft_model_state_dict\n",
    "def get_peft_state_maybe_zero_3(named_params, bias):\n",
    "    if bias == \"none\":\n",
    "        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n",
    "    elif bias == \"all\":\n",
    "        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n",
    "    elif bias == \"lora_only\":\n",
    "        to_return = {}\n",
    "        maybe_lora_bias = {}\n",
    "        lora_bias_names = set()\n",
    "        for k, t in named_params:\n",
    "            if \"lora_\" in k:\n",
    "                to_return[k] = t\n",
    "                bias_name = k.split(\"lora_\")[0] + \"bias\"\n",
    "                lora_bias_names.add(bias_name)\n",
    "            elif \"bias\" in k:\n",
    "                maybe_lora_bias[k] = t\n",
    "        for k, t in maybe_lora_bias:\n",
    "            if bias_name in lora_bias_names:\n",
    "                to_return[bias_name] = t\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True) for k, v in to_return.items()}\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def get_peft_state_non_lora_maybe_zero_3(named_params, require_grad_only=True):\n",
    "    to_return = {k: t for k, t in named_params if \"lora_\" not in k}\n",
    "    if require_grad_only:\n",
    "        to_return = {k: t for k, t in to_return.items() if t.requires_grad}\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def get_mm_adapter_state_maybe_zero_3(named_params, keys_to_match):\n",
    "    to_return = {k: t for k, t in named_params if any(key_match in k for key_match in keys_to_match)}\n",
    "    to_return = {k: maybe_zero_3(v, ignore_status=True).cpu() for k, v in to_return.items()}\n",
    "    return to_return\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['mm_projector', 'vision_tower', 'vision_resampler']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "\n",
    "    if getattr(trainer.args, \"tune_mm_mlp_adapter\", False):\n",
    "        # Only save Adapter\n",
    "        keys_to_match = ['mm_projector']\n",
    "        if getattr(trainer.args, \"use_im_start_end\", False):\n",
    "            keys_to_match.extend(['embed_tokens', 'embed_in'])\n",
    "\n",
    "        weight_to_save = get_mm_adapter_state_maybe_zero_3(trainer.model.named_parameters(), keys_to_match)\n",
    "        trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "        current_folder = output_dir.split('/')[-1]\n",
    "        parent_folder = os.path.dirname(output_dir)\n",
    "        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:\n",
    "            if current_folder.startswith('checkpoint-'):\n",
    "                mm_projector_folder = os.path.join(parent_folder, \"mm_projector\")\n",
    "                os.makedirs(mm_projector_folder, exist_ok=True)\n",
    "                torch.save(weight_to_save, os.path.join(mm_projector_folder, f'{current_folder}.bin'))\n",
    "            else:\n",
    "                torch.save(weight_to_save, os.path.join(output_dir, f'mm_projector.bin'))\n",
    "        return\n",
    "\n",
    "    if trainer.deepspeed:\n",
    "        torch.cuda.synchronize()\n",
    "        trainer.save_model(output_dir)\n",
    "        return\n",
    "\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu()\n",
    "            for key, value in state_dict.items()\n",
    "        }\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def save_my_lora_ckpt(output_dir, args, model):\n",
    "    state_dict = get_peft_state_maybe_zero_3(\n",
    "        model.named_parameters(), args.lora_bias\n",
    "    )\n",
    "    non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "        model.named_parameters()\n",
    "    )\n",
    "    if args.local_rank == 0 or args.local_rank == -1:\n",
    "        model.config.save_pretrained(output_dir)\n",
    "        model.save_pretrained(output_dir, state_dict=state_dict)\n",
    "        torch.save(non_lora_state_dict, os.path.join(output_dir, 'non_lora_trainables.bin'))\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7903199e-f18d-4f49-a241-e97ee84f0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_conv(prompt, answer):\n",
    "    return [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": prompt,\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": answer,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def preprocess_v1(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    X: str = None\n",
    ") -> Dict:\n",
    "    conv = conversation_lib.default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "\n",
    "    if X is not None:\n",
    "        input_ids = torch.stack([tokenizer_X_token(prompt, tokenizer, X_TOKEN_INDEX[X], return_tensors='pt') for prompt in conversations], dim=0)\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            conversations,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ).input_ids\n",
    "\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == conversation_lib.SeparatorStyle.TWO\n",
    "\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "\n",
    "            if X is not None:\n",
    "                round_len = len(tokenizer_X_token(rou, tokenizer, X_TOKEN_INDEX[X]))\n",
    "                instruction_len = len(tokenizer_X_token(parts[0], tokenizer, X_TOKEN_INDEX[X])) - 2\n",
    "            else:\n",
    "                round_len = len(tokenizer(rou).input_ids)\n",
    "                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\n",
    "                round_len -= 1\n",
    "                instruction_len -= 1\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_INDEX\n",
    "                print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_plain(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    X: str\n",
    ") -> Dict:\n",
    "    DEFAULT_TOKEN = DEFAULT_X_TOKEN[X]\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_TOKEN in source[0]['value']\n",
    "        source[0]['value'] = DEFAULT_TOKEN\n",
    "        conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\n",
    "        conversations.append(conversation)\n",
    "    # tokenize conversations\n",
    "    input_ids = [tokenizer_X_token(prompt, tokenizer, X_TOKEN_INDEX[X], return_tensors='pt') for prompt in conversations]\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_len = len(tokenizer_X_token(source[0]['value'], tokenizer, X_TOKEN_INDEX[X]))\n",
    "        target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_X: str = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    X = has_X if has_X is None else has_X.upper()\n",
    "    if conversation_lib.default_conversation.sep_style == conversation_lib.SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain(sources, tokenizer, X=X)\n",
    "    elif conversation_lib.default_conversation.version.startswith(\"v1\"):\n",
    "        return preprocess_v1(sources, tokenizer, X=X)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def load_data(data_args):\n",
    "    if 'jsonl' in data_args.data_path:\n",
    "        data_list = load_jsonl(data_args.data_path)\n",
    "    else: \n",
    "        data_list = load_json(data_args.data_path)\n",
    "    return data_list\n",
    "\n",
    "class DPODataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "        super(Dataset, self).__init__()\n",
    "        list_data_dict = load_data(data_args)\n",
    "        if data_args.num_sample is not None:\n",
    "            list_data_dict = list_data_dict[:data_args.num_sample]\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.data_args = data_args\n",
    "        self.training_modal = data_args.training_modal\n",
    "\n",
    "    def __len__(self):\n",
    "        # return 20\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            img_tokens = 128 if any([x.lower() in sample for x in DEFAULT_X_TOKEN.keys()]) else 0\n",
    "            length_list.append(sum(len(conv['value'].split()) for conv in sample['conversations']) + img_tokens)\n",
    "        return length_list\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n",
    "            cur_len = cur_len if any([x.lower() in sample for x in DEFAULT_X_TOKEN.keys()]) else -cur_len\n",
    "            length_list.append(cur_len)\n",
    "        return length_list\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "        {\n",
    "            'prompt': 'Is there a snowman wearing a green scarf and hat in the background?',\n",
    "            'chosen': 'No, there is no snowman wearing a green scarf and hat in the background of the image. The image features a person ...',\n",
    "            'rejected': 'No, there is no snowman in the background.',\n",
    "            'image_path': '/mnt/bn/liangkeg/data/ruohongz/dpo_data/dpo_images/LRVInstruction-000000009569.jpg',\n",
    "            'image_name': 'LRVInstruction-000000009569.jpg'\n",
    "        }\n",
    "        '''\n",
    "        try:\n",
    "            has_X = None\n",
    "            # sources = self.list_data_dict[i]\n",
    "            # if isinstance(i, int):\n",
    "            #     sources = [sources]\n",
    "            # assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "            data_dict = copy.deepcopy(self.list_data_dict[i]) # inplace modification following\n",
    "            if self.training_modal == 'image':\n",
    "                image_file = data_dict['frame']\n",
    "                image_folder = self.data_args.image_folder\n",
    "                processor = self.data_args.image_processor\n",
    "                image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "                if self.data_args.image_aspect_ratio == 'pad':\n",
    "                    def expand2square(pil_img, background_color):\n",
    "                        width, height = pil_img.size\n",
    "                        if width == height:\n",
    "                            return pil_img\n",
    "                        elif width > height:\n",
    "                            result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                            result.paste(pil_img, (0, (width - height) // 2))\n",
    "                            return result\n",
    "                        else:\n",
    "                            result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                            result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                            return result\n",
    "                    image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "                else:\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "                prompt = data_dict['prompt']\n",
    "                prompt = prompt.replace(\"<image>\", \"\").strip()\n",
    "                prompt = \"<image>\\n\" + prompt\n",
    "                data_dict['prompt'] = prompt\n",
    "                has_X = 'image'\n",
    "\n",
    "            elif self.training_modal == 'video':\n",
    "                video_file = data_dict['video']\n",
    "                video_folder = self.data_args.video_folder\n",
    "                processor = self.data_args.video_processor\n",
    "                video = os.path.join(video_folder, video_file)\n",
    "                # print(video)\n",
    "                video = processor(video, return_tensors='pt')['pixel_values'][0]\n",
    "                # print(video, 'success')\n",
    "                # sources = preprocess_multimodal(make_conversation([e[\"detail\"] for e in sources]), self.data_args)\n",
    "                prompt = data_dict['prompt']\n",
    "                prompt = prompt.replace(\"<video>\", \"\").strip()\n",
    "                prompt = \"<video>\\n\" + prompt\n",
    "                data_dict['prompt'] = prompt\n",
    "                has_X = 'video'\n",
    "            else:\n",
    "                raise(\"Training modal not supported\")\n",
    "                # prompt = data_dict['prompt']\n",
    "                # has_X = 'image' # not used, placeholder below\n",
    "                # image = torch.zeros(3, 224, 224)\n",
    "\n",
    "            data_dict['has_X'] = has_X\n",
    "            if has_X == 'image':\n",
    "                data_dict['image'] = image\n",
    "            elif has_X == 'video':\n",
    "                data_dict['video'] = video\n",
    "                # print('success video')\n",
    "            \n",
    "            return data_dict\n",
    "        except Exception as e:\n",
    "            print(f'Error with {e}, {self.list_data_dict[i]}')\n",
    "            return self.__getitem__(random.randint(0, self.__len__()-1))\n",
    "\n",
    "@dataclass\n",
    "class DPODataCollator(DPODataCollatorWithPadding):\n",
    "    def collate(self, batch):\n",
    "        # first, pad everything to the same length\n",
    "        # input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "        #                           for key in (\"input_ids\", \"labels\"))\n",
    "        # input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        #     input_ids,\n",
    "        #     batch_first=True,\n",
    "        #     padding_value=self.tokenizer.pad_token_id)\n",
    "        # labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "        #                                          batch_first=True,\n",
    "        #                                          padding_value=IGNORE_INDEX)\n",
    "        # input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        # labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        # batch = dict(\n",
    "        #     input_ids=input_ids,\n",
    "        #     labels=labels,\n",
    "        #     attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        # )\n",
    "        padded_batch = {}\n",
    "        for k in batch[0].keys():\n",
    "            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n",
    "                # if \"prompt\" in k:\n",
    "                #     to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]\n",
    "                # else:\n",
    "                to_pad = [torch.LongTensor(ex[k]) for ex in batch]\n",
    "                if k.endswith(\"_input_ids\"):\n",
    "                    padding_value = self.tokenizer.pad_token_id\n",
    "                elif k.endswith(\"_labels\"):\n",
    "                    padding_value = self.label_pad_token_id\n",
    "                else:\n",
    "                    continue\n",
    "                # elif k.endswith(\"_attention_mask\"):\n",
    "                #     padding_value = self.padding_value\n",
    "                # else:\n",
    "                #     raise ValueError(f\"Unexpected key in batch '{k}'\")\n",
    "\n",
    "                padded_batch[k] = torch.nn.utils.rnn.pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n",
    "                # for the prompt, flip back so padding is on left side\n",
    "                # if \"prompt\" in k:\n",
    "                #     padded_batch[k] = padded_batch[k].flip(dims=[1])\n",
    "            else:\n",
    "                padded_batch[k] = [ex[k] for ex in batch]\n",
    "        for k in ['chosen_input_ids', 'rejected_input_ids']:\n",
    "            attn_k = k.replace('input_ids', 'attention_mask')\n",
    "            padded_batch[attn_k] = padded_batch[k].ne(self.tokenizer.pad_token_id)\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def tokenize_batch_element(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        chosen: str,\n",
    "        rejected: str,\n",
    "        has_X: str = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Tokenize a single batch element.\n",
    "\n",
    "        At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n",
    "            in case the prompt + chosen or prompt + rejected responses is/are too long. First\n",
    "            we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n",
    "\n",
    "        We also create the labels for the chosen/rejected responses, which are of length equal to\n",
    "            the sum of the length of the prompt and the chosen/rejected response, with\n",
    "            label_pad_token_id  for the prompt tokens.\n",
    "        \"\"\"\n",
    "        # import pdb; pdb.set_trace()\n",
    "        batch = {}\n",
    "        \n",
    "        chosen_sources = make_conv(prompt, chosen)\n",
    "        rejected_sources = make_conv(prompt, rejected)\n",
    "        chosen_data_dict = preprocess(\n",
    "            [chosen_sources],\n",
    "            self.tokenizer,\n",
    "            has_X=has_X\n",
    "        )\n",
    "        #chosen_data_dict['attention_mask'] = chosen_data_dict[\"input_ids\"].ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "        rejected_data_dict = preprocess(\n",
    "            [rejected_sources],\n",
    "            self.tokenizer,\n",
    "            has_X=has_X\n",
    "        )\n",
    "        #rejected_data_dict['attention_mask'] = rejected_data_dict[\"input_ids\"].ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "        chosen_data_dict = {k: v[0] for k, v in chosen_data_dict.items()}\n",
    "        rejected_data_dict = {k: v[0] for k, v in rejected_data_dict.items()}\n",
    "\n",
    "        for k, toks in {\n",
    "            \"chosen\": chosen_data_dict,\n",
    "            \"rejected\": rejected_data_dict,\n",
    "        }.items():\n",
    "            for type_key, tokens in toks.items():\n",
    "                if type_key == \"token_type_ids\":\n",
    "                    continue\n",
    "                batch[f\"{k}_{type_key}\"] = tokens\n",
    "        return batch\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        tokenized_batch = []\n",
    "        Xs, keys = [], []\n",
    "        for feature in features:\n",
    "            prompt = feature[\"prompt\"]\n",
    "            chosen = feature[\"chosen\"]\n",
    "            rejected = feature[\"rejected\"]\n",
    "            has_X = feature['has_X']\n",
    "            Xs.append(feature[has_X])\n",
    "            keys.append(has_X)\n",
    "             \n",
    "            batch_element = self.tokenize_batch_element(prompt, chosen, rejected, has_X=has_X)\n",
    "            tokenized_batch.append(batch_element)\n",
    "\n",
    "        # return collated batch\n",
    "        padded_batch =  self.collate(tokenized_batch)\n",
    "        padded_batch['images'] = [Xs, keys]  # we do not change the key's name.\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "def make_dpo_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = DPODataset(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path,\n",
    "                                data_args=data_args)\n",
    "    return train_dataset\n",
    "    # data_collator = DataCollatorForDPODataset(tokenizer=tokenizer)\n",
    "    # return dict(train_dataset=train_dataset,\n",
    "    #             eval_dataset=None,\n",
    "    #             data_collator=data_collator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4063e-d4c9-4ad6-9b98-26e55a4ec9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(attn_implementation):\n",
    "    global local_rank\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    local_rank = training_args.local_rank\n",
    "\n",
    "    if training_args.bits in [4, 8]:\n",
    "        raise NotImplementedError(\"Quantization is not supported yet.\")\n",
    "    \n",
    "    if model_args.image_tower is not None or model_args.video_tower is not None:  ###################################################\n",
    "        model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "        )\n",
    "    else:\n",
    "        model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=(torch.bfloat16 if training_args.bf16 else None),\n",
    "        )\n",
    "    model.config.use_cache = False\n",
    "    model.config.X = model_args.X\n",
    "\n",
    "    if model_args.freeze_backbone:\n",
    "        model.model.requires_grad_(False)\n",
    "\n",
    "    # if not training_args.use_lora:\n",
    "    #     if (\n",
    "    #         training_args.fix_vit\n",
    "    #         and hasattr(model, \"transformer\")\n",
    "    #         and hasattr(model.transformer, \"visual\")\n",
    "    #     ):\n",
    "    #         model.transformer.visual.requires_grad_(False)\n",
    "    #         if hasattr(model.transformer.visual, \"attn_pool\"):\n",
    "    #             model.transformer.visual.attn_pool.requires_grad_(True)\n",
    "\n",
    "    if training_args.gradient_checkpointing:\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "        else:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    if training_args.lora_enable:\n",
    "        logger.info(f\"init peft model\")\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        lora_config = LoraConfig(\n",
    "            r=training_args.lora_r,\n",
    "            lora_alpha=training_args.lora_alpha,\n",
    "            target_modules=find_all_linear_names(model),\n",
    "            lora_dropout=training_args.lora_dropout,\n",
    "            bias=training_args.lora_bias,\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        if training_args.bits == 16:\n",
    "            if training_args.bf16:\n",
    "                model.to(torch.bfloat16)\n",
    "            if training_args.fp16:\n",
    "                model.to(torch.float16)\n",
    "        rank0_print(\"Adding LoRA adapters...\")\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    if model_args.version == \"v0\":\n",
    "        if tokenizer.pad_token is None:\n",
    "            smart_tokenizer_and_embedding_resize(\n",
    "                special_tokens_dict=dict(pad_token=\"[PAD]\"),\n",
    "                tokenizer=tokenizer,\n",
    "                model=model,\n",
    "            )\n",
    "    elif model_args.version == \"v0.5\":\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "    else:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        if model_args.version in conversation_lib.conv_templates:\n",
    "            conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n",
    "        else:\n",
    "            conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n",
    "\n",
    "    if model_args.image_tower is not None or model_args.video_tower is not None:  #############################\n",
    "        if model_args.image_tower is not None:\n",
    "            image_tower = model.get_image_tower()\n",
    "            if image_tower is None:\n",
    "                model.get_model().initialize_image_modules(\n",
    "                    model_args=model_args,\n",
    "                    fsdp=training_args.fsdp\n",
    "                )\n",
    "                image_tower = model.get_image_tower()\n",
    "            if not image_tower.is_loaded:\n",
    "                # print('load image tower')\n",
    "                image_tower.load_model()\n",
    "            image_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "            data_args.image_processor = image_tower.image_processor\n",
    "            data_args.is_multimodal = True\n",
    "\n",
    "            model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "            model.config.image_grid_pinpoints = data_args.image_grid_pinpoints\n",
    "\n",
    "        if model_args.video_tower is not None:\n",
    "            video_tower = model.get_video_tower()\n",
    "            if video_tower is None:\n",
    "                model.get_model().initialize_video_modules(\n",
    "                    model_args=model_args,\n",
    "                    fsdp=training_args.fsdp\n",
    "                )\n",
    "                video_tower = model.get_video_tower()\n",
    "            if not video_tower.is_loaded:\n",
    "                # print('load video tower')\n",
    "                video_tower.load_model()\n",
    "            video_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "            data_args.video_processor = video_tower.video_processor\n",
    "            data_args.is_multimodal = True\n",
    "\n",
    "            # model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "            # model.config.image_grid_pinpoints = data_args.image_grid_pinpoints\n",
    "\n",
    "        model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "        if model_args.tune_mm_mlp_adapter:\n",
    "            model.requires_grad_(False)\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "        if training_args.freeze_mm_mlp_adapter:\n",
    "            for p in model.get_model().mm_projector.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        model.config.mm_use_x_start_end = data_args.mm_use_x_start_end = model_args.mm_use_x_start_end\n",
    "        training_args.use_x_start_end = model_args.mm_use_x_start_end\n",
    "        model.config.mm_use_x_patch_token = model_args.mm_use_x_patch_token\n",
    "        model.initialize_X_tokenizer(model_args, tokenizer=tokenizer)\n",
    "\n",
    "    ###################\n",
    "    # for p in model.get_model().layers.parameters():\n",
    "    #     p.requires_grad = False\n",
    "    # for p in model.get_model().norm.parameters():\n",
    "    #     p.requires_grad = False\n",
    "    # for p in model.get_model().embed_tokens.parameters():\n",
    "    #     p.requires_grad = False\n",
    "    # for p in model.lm_head.parameters():\n",
    "    #     p.requires_grad = False\n",
    "    #################\n",
    "\n",
    "    train_dataset = make_dpo_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    data_collator = DPODataCollator(\n",
    "            tokenizer,\n",
    "            label_pad_token_id=IGNORE_INDEX,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # dict(train_dataset=train_dataset,\n",
    "    #             eval_dataset=None,\n",
    "    #             data_collator=data_collator)\n",
    "\n",
    "    #################\n",
    "    # print(f'{training_args}')\n",
    "    # print(f'{model_args}')\n",
    "    # for n, p in model.named_parameters():\n",
    "    #     if p.requires_grad:\n",
    "    #         print(f'{n}==>{p.requires_grad}')\n",
    "    # print(f'{model}')\n",
    "    # print(model.device)\n",
    "    #################\n",
    "\n",
    "    # data_collator = data_module['data_collator']\n",
    "    # train_dataset = data_module['train_dataset']\n",
    "    trainer = LLaVADPOTrainer(\n",
    "        model,\n",
    "        args=training_args,\n",
    "        dpo_alpha=training_args.dpo_alpha,\n",
    "        beta=training_args.beta,\n",
    "        gamma=training_args.gamma,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=training_args.model_max_length,\n",
    "        generate_during_eval=False, #training_args.generate_during_eval,\n",
    "    )\n",
    "    trainer.save_my_lora_ckpt = save_my_lora_ckpt\n",
    "    # trainer = LLaVATrainer(model=model,\n",
    "    #                 tokenizer=tokenizer,\n",
    "    #                 args=training_args,\n",
    "    #                 **data_module)\n",
    "\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    trainer.save_state()\n",
    "\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    if training_args.lora_enable:\n",
    "        state_dict = get_peft_state_maybe_zero_3(\n",
    "            model.named_parameters(), training_args.lora_bias\n",
    "        )\n",
    "        non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(\n",
    "            model.named_parameters()\n",
    "        )\n",
    "        if training_args.local_rank == 0 or training_args.local_rank == -1:\n",
    "            model.config.save_pretrained(training_args.output_dir)\n",
    "            model.save_pretrained(training_args.output_dir, state_dict=state_dict)\n",
    "            torch.save(non_lora_state_dict, os.path.join(training_args.output_dir, 'non_lora_trainables.bin'))\n",
    "    else:\n",
    "        safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                       output_dir=training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(attn_implementation=\"flash_attention_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f253c2-eeb0-4442-9e18-9ceeac95a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dpo_data_module(tokenizer=tokenizer, data_args=data_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
